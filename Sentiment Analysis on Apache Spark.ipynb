{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import re\n",
    "import random \n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import seaborn as sn\n",
    "\n",
    "import time\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "import zoo\n",
    "from zoo.common.nncontext import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@udf(\"int\")\n",
    "def set_label(sentiment):\n",
    "    if sentiment==4:\n",
    "        return 1\n",
    "    elif sentiment==0:\n",
    "        return 0\n",
    "@udf('int')\n",
    "def length_udf(text):\n",
    "    return len(text.split(' '))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sentiment\", IntegerType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"query\", StringType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"row text\", StringType())\n",
    "])\n",
    "\n",
    "df_main = spark.read.csv('../workTable/dataset/sentiment140-raw.csv', header = False, schema=schema)\n",
    "df_main = df_main.withColumn('label', set_label(df_main.sentiment))\n",
    "del_columns = ['id', 'date', 'query', 'user', 'sentiment']\n",
    "df_main = df_main.drop(*del_columns)\n",
    "df_main = df_main.na.drop()\n",
    "df_main = df_main.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "df_count = df_main.count()\n",
    "print(\"Dataset count: \"+str(df_count))\n",
    "df_main.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = \"~/workTable/dataset/wiki-news-300d-1M-subword.vec\"\n",
    "ft_model = KeyedVectors.load_word2vec_format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "m = df_count\n",
    "range_ = (int)(m/n) + 1\n",
    "split_point = []\n",
    "for i in range(1, range_):\n",
    "    t = (i * n) / m\n",
    "    split_point.append(t)\n",
    "    \n",
    "\n",
    "print('Split Point size: '+str(len(split_point)))\n",
    "print(split_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(review_text):\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    return words\n",
    "\n",
    "def analyze_texts(data_rdd):\n",
    "    def index(w_c_i):\n",
    "        ((w, c), i) = w_c_i\n",
    "        return (w, (i + 1, c))\n",
    "    return data_rdd.flatMap(lambda text_label: text_to_words(text_label[0])) \\\n",
    "        .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda w_c: - w_c[1]).zipWithIndex() \\\n",
    "        .map(lambda w_c_i: index(w_c_i)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data_rdd,\n",
    "                    sequence_len,\n",
    "                    embedding_dim,\n",
    "                    bword_to_ic,\n",
    "                    bfiltered_w2v):\n",
    "    \n",
    "    def pad(l, fill_value, width):\n",
    "        if len(l) >= width:\n",
    "            return l[0: width]\n",
    "        else:\n",
    "            l.extend([fill_value] * (width - len(l)))\n",
    "            return l\n",
    "    \n",
    "    def to_vec(token, b_w2v, embedding_dim):\n",
    "\n",
    "        if token in b_w2v:\n",
    "            return b_w2v[token]\n",
    "        else:\n",
    "            return pad([], 0, embedding_dim) \n",
    "        \n",
    "    def to_sample(vectors, label, embedding_dim,sequence_len):\n",
    "        flatten_features = list(itertools.chain(*vectors)) # flatten nested list\n",
    "        features = np.array(flatten_features, dtype='float').reshape(\n",
    "            [sequence_len, embedding_dim])\n",
    "        \n",
    "        return Sample.from_ndarray(features, np.array(label))\n",
    "    \n",
    "    \n",
    "    tokens_rdd = data_rdd.map(lambda text_label:\n",
    "                              ([w for w in text_to_words(text_label[0]) if\n",
    "                                w in bword_to_ic.value], text_label[1]))\n",
    "    \n",
    "    \n",
    "    padded_tokens_rdd = tokens_rdd.map(\n",
    "        lambda tokens_label: (pad(tokens_label[0], \"##\", sequence_len), tokens_label[1]))\n",
    "\n",
    "    \n",
    "    vector_rdd = padded_tokens_rdd.map(lambda tokens_label:\n",
    "                                       ([to_vec(w, bfiltered_w2v.value,\n",
    "                                                embedding_dim) for w in\n",
    "                                         tokens_label[0]], tokens_label[1]))\n",
    "    \n",
    "#     sample_rdd = vector_rdd.map(\n",
    "#         lambda vectors_label: to_sample(vectors_label[0], vectors_label[1], embedding_dim, sequence_len))\n",
    "\n",
    "    return vector_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9\\_]+_+[\\_A-Za-z0-9]+'\n",
    "pat2 = r'@[A-Za-z0-9\\_]+'\n",
    "pat3 = r'https*://[A-Za-z0-9./]+'\n",
    "pat4 = r'www*.[A-Za-z0-9]+\\.+[A-Za-z0-9]+\\.+[A-Za-z0-9]+'\n",
    "pat5 = r'^\\s'\n",
    "pat6 = r'www.[^ ]+'\n",
    "\n",
    "combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5,pat6))\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                             \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                             \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                             \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\n",
    "                             \"mightn't\":\"might not\",\"mustn't\":\"must not\",\"i'm\":\"i am\",\"you're\":\"you are\",\n",
    "                             \"they've\":\"they have\",\"we'll\":\"we will\",\"i'll\":\"i will\",\"it's\":\"it is\",\n",
    "                             \"you've\":\"you have\",\"i've\":\"i have\",\"he's\":\"he is\", \"that's\":\"that is\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(combined_pat,negations_dic ,neg_pattern ,text):            \n",
    "            \n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    \n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub('  +', ' ', stripped)\n",
    "    lower_case = stripped.lower()            \n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "            \n",
    "    suggestions = sym_spell.lookup_compound(letters_only, max_edit_distance=2)\n",
    "        \n",
    "    for suggestion in suggestions:\n",
    "        corrected_sentence = suggestion.term\n",
    "            \n",
    "    words = [x for x  in tok.tokenize(corrected_sentence) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "# from spellchecker import SpellChecker\n",
    "# from autocorrect import Speller\n",
    "\n",
    "# Speller = Speller(lang='en')\n",
    "\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "class TwittCleaner(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != StringType()):\n",
    "            raise Exception('Input type %s did not match input type TimestampType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "\n",
    "        def tweet_cleaner_updated(text):\n",
    "            \n",
    "            tok = WordPunctTokenizer()\n",
    "\n",
    "            pat1 = r'@[A-Za-z0-9\\_]+_+[\\_A-Za-z0-9]+'\n",
    "            pat2 = r'@[A-Za-z0-9\\_]+'\n",
    "            pat3 = r'https*://[A-Za-z0-9./]+'\n",
    "            pat4 = r'www*.[A-Za-z0-9]+\\.+[A-Za-z0-9]+\\.+[A-Za-z0-9]+'\n",
    "            pat5 = r'^\\s'\n",
    "            pat6 = r'www.[^ ]+'\n",
    "\n",
    "            combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5,pat6))\n",
    "\n",
    "            negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                             \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                             \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                             \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\n",
    "                             \"mightn't\":\"might not\",\"mustn't\":\"must not\",\"i'm\":\"i am\",\"you're\":\"you are\",\n",
    "                             \"they've\":\"they have\",\"we'll\":\"we will\",\"i'll\":\"i will\",\"it's\":\"it is\",\n",
    "                             \"you've\":\"you have\",\"i've\":\"i have\",\"he's\":\"he is\", \"that's\":\"that is\"}\n",
    "            neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "            \n",
    "            \n",
    "            soup = BeautifulSoup(text, 'lxml')\n",
    "            souped = soup.get_text()\n",
    "            try:\n",
    "                bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "            except:\n",
    "                bom_removed = souped\n",
    "    \n",
    "#             word_list = bom_removed.split(' ')\n",
    "            \n",
    "#             for index,value in enumerate(word_list):\n",
    "#                 #word_list[index] = spell.correction(value)\n",
    "#                 word_list[index] = spell(value)\n",
    "    \n",
    "#             corrected_sentence = Speller(bom_removed)\n",
    "    \n",
    "            stripped = re.sub(combined_pat, '', bom_removed)\n",
    "            stripped = re.sub('  +', ' ', stripped)\n",
    "            lower_case = stripped.lower()            \n",
    "            neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "            letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "            \n",
    "            suggestions = sym_spell.lookup_compound(letters_only, max_edit_distance=2)\n",
    "        \n",
    "            for suggestion in suggestions:\n",
    "                corrected_sentence = suggestion.term\n",
    "            \n",
    "            words = [x for x  in tok.tokenize(corrected_sentence) if len(x) > 1]\n",
    "            return (\" \".join(words)).strip()        \n",
    "        in_col = df[self.inputCol]\n",
    "        \n",
    "        return df.withColumn(self.outputCol, udf(tweet_cleaner_updated, StringType())(in_col))\n",
    "    \n",
    "class RPadd(Transformer):\n",
    "    def __init__(self, inputCol, outputCol, sequence_len, fill_value):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.sequence_len = sequence_len\n",
    "        self.fill_value = fill_value\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != ArrayType()):\n",
    "            raise Exception('Input type %s did not match input type TimestampType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):        \n",
    "        \n",
    "        data = df[self.inputCol]\n",
    "        \n",
    "        def rpad(words, length = self.sequence_len, fill_value = self.fill_value):\n",
    "            if len(words) >= length:\n",
    "                return words[0: length]\n",
    "            else:\n",
    "                words.extend([fill_value] * (length - len(words)))\n",
    "                return words\n",
    "\n",
    "        return df.withColumn(self.outputCol, udf(rpad, ArrayType(StringType()))(data))\n",
    "\n",
    "class Vectorized(Transformer):\n",
    "    def __init__(self, inputCol, outputCol, wv):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.wv = wv\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != ArrayType()):\n",
    "            raise Exception('Input type %s did not match input type TimestampType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):        \n",
    "        \n",
    "        data = df[self.inputCol]\n",
    "        \n",
    "        def vec_udf(words, b_w2v = self.wv, embedding_dim = embedding_dim):\n",
    "            vectors = []\n",
    "            for w in words:\n",
    "                if w in b_w2v:\n",
    "                    vectors.append(b_w2v[w])\n",
    "                else:\n",
    "                    \n",
    "                    def pad(l, fill_value, width):\n",
    "                        if len(l) >= width:\n",
    "                            return l[0: width]\n",
    "                        else:\n",
    "                            l.extend([fill_value] * (width - len(l)))\n",
    "                            return l\n",
    "                    \n",
    "                    vectors.append(pad([], 0, embedding_dim))\n",
    "            \n",
    "            \n",
    "            flatten_features = list(itertools.chain(*vectors))\n",
    "            features = np.array(flatten_features, dtype='float').reshape([sequence_len, embedding_dim])\n",
    "            \n",
    "            v = Vectors.dense(features.flatten())\n",
    "            \n",
    "            return v\n",
    "\n",
    "        return df.withColumn(self.outputCol, udf(vec_udf, VectorUDT())(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic Version (RDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rdd_time = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(10, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    s1 = time.time()\n",
    "    \n",
    "    df_rdd = df_main.orderBy(rand()).sample(False, split_point[i], 10).collect()\n",
    "    COUNT = len(df_rdd)\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    \n",
    "    print('   RDD is created ...')\n",
    "    \n",
    "    s2 = time.time()\n",
    "    cleaned_rdd = data_rdd.map(lambda row: (tweet_cleaner_updated(combined_pat, negations_dic, neg_pattern, row[1])\n",
    "                                            , row[0]))    \n",
    "    b = cleaned_rdd.collect()    \n",
    "    e2 = time.time() - s2\n",
    "    print('   RDD is cleaned! ===> time:' + str(e2))\n",
    "\n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "\n",
    "    for row in b:    \n",
    "        if row[0] is not None:    \n",
    "            word_list = row[0].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(cleaned_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    \n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1) + '-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1) + '-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "    \n",
    "    s3 = time.time()\n",
    "    sample_rdd = prepare_samples(cleaned_rdd, sequence_len, embedding_dim, bword_to_ic, bfiltered_w2v)\n",
    "    b = sample_rdd.collect()    \n",
    "    e3 = time.time() - s3\n",
    "    \n",
    "    print('   Done!')\n",
    "    print('   Executaion Time: ' + str(e3) + ' s ==> completed!')\n",
    "\n",
    "    rdd_time[COUNT] = e3\n",
    "    \n",
    "    e1 = time.time() - s1\n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sorted Version (RDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rdd_time_sorted= {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    s1 = time.time()\n",
    "    \n",
    "    df_rdd = df_main.orderBy(rand())\\\n",
    "        .sample(False, split_point[i], 10)\\\n",
    "        .rdd.sortBy(lambda line: len(line[1].split(' ')) if len(line[1]) > 1 else None).collect()\n",
    "    \n",
    "    COUNT = len(df_rdd)\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    \n",
    "    s2 = time.time()\n",
    "    cleaned_rdd = data_rdd.map(lambda row: (tweet_cleaner_updated(combined_pat, negations_dic, neg_pattern, row[1])\n",
    "                                            , row[0]))\n",
    "    b = cleaned_rdd.collect()\n",
    "    e2 = time.time() - s2\n",
    "\n",
    "    print('   RDD is cleaned! ===> time:' + str(e2))\n",
    "\n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "\n",
    "    for row in b:    \n",
    "        if row[0] is not None:    \n",
    "            word_list = row[0].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(cleaned_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    \n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1) + '-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1) + '-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "    \n",
    "    s3 = time.time()\n",
    "    sample_rdd = prepare_samples(cleaned_rdd, sequence_len, embedding_dim, bword_to_ic, bfiltered_w2v)\n",
    "    b = sample_rdd.collect()    \n",
    "    e3 = time.time() - s3\n",
    "    \n",
    "    print('   Done!')\n",
    "    print('   Executaion Time: ' + str(e3) + ' s ==> completed!')\n",
    "\n",
    "    rdd_time_sorted[COUNT] = e3\n",
    "    \n",
    "    e1 = time.time() - s1\n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic (Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_time = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_rdd = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    COUNT = df_rdd.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    s1 = time.time()\n",
    "    tc = TwittCleaner(inputCol='row text', outputCol='cleaned_text')\n",
    "    pipeline = Pipeline(stages=[tc])\n",
    "    pipiline_model = pipeline.fit(df_rdd)\n",
    "    df_rdd = pipiline_model.transform(df_rdd)\n",
    "    df_rdd.show(5)\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    df_time[COUNT] = e1\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_time2 = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    s1 = time.time()\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    df_rdd = df_sample.collect()\n",
    "    COUNT = df_sample.count()\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    \n",
    "    print('   RDD is created ...')\n",
    "    \n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "\n",
    "    for row in df_rdd:    \n",
    "        if row[2] is not None:    \n",
    "            word_list = row[2].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(data_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1)+'-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1)+'-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "    \n",
    "    \n",
    "    s2 = time.time()\n",
    "    w = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    p = RPadd('words', 'padded', sequence_len, '##')\n",
    "    v = Vectorized('padded', 'vectorized', bfiltered_w2v.value)\n",
    "    pipeline = Pipeline(stages=[w, p, v])\n",
    "    pipiline_model = pipeline.fit(df_sample)\n",
    "    df_sample = pipiline_model.transform(df_sample)\n",
    "    df_sample.show(5)\n",
    "    e2 = time.time() - s2\n",
    "\n",
    "    print('   Done!')\n",
    "    print('   Executaion Time: ' + str(e2) + ' s ==> completed!')\n",
    "\n",
    "    df_time2[COUNT] = e2\n",
    "    \n",
    "    e1 = time.time() - s1\n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorted Version (Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sorted_df_time1 = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    \n",
    "    df_sample = df_sample.withColumn('text_length', length_udf(df_sample['row text']))\n",
    "\n",
    "    df_sample = df_sample.sort(df_sample['text_length'])\n",
    "    \n",
    "    COUNT = df_sample.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    s1 = time.time()\n",
    "    df_sample = df_sample.sort(df_sample['text_length'])\n",
    "    tc = TwittCleaner(inputCol='row text', outputCol='cleaned_text')\n",
    "    pipeline = Pipeline(stages=[tc])\n",
    "    pipiline_model = pipeline.fit(df_sample)\n",
    "    df_sample = pipiline_model.transform(df_sample)\n",
    "    df_sample.show(5)\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    sorted_df_time[COUNT] = e1\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sorted_df_time2 = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')    \n",
    "    s1 = time.time()    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    df_sample = df_sample.withColumn('text_length', length_udf(df_sample.text))\n",
    "    df_sample = df_sample.sort(df_sample['text_length'])    \n",
    "    df_rdd = df_sample.collect()\n",
    "    COUNT = df_sample.count()\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    print('   DF count: ' + str(COUNT))    \n",
    "    print('   RDD is created ...')    \n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "    for row in df_rdd:    \n",
    "        if row[2] is not None:    \n",
    "            word_list = row[2].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(data_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1)+'-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1)+'-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "    \n",
    "    \n",
    "    s2 = time.time()\n",
    "    w = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    p = RPadd('words', 'padded', sequence_len, '##')\n",
    "    v = Vectorized('padded', 'vectorized', bfiltered_w2v.value)\n",
    "    pipeline = Pipeline(stages=[w, p, v])\n",
    "    pipiline_model = pipeline.fit(df_sample)\n",
    "    df_sample = pipiline_model.transform(df_sample)\n",
    "    df_sample.show(5)\n",
    "    e2 = time.time() - s2\n",
    "\n",
    "    print('   Done!')\n",
    "    print('   Executaion Time: ' + str(e2) + ' s ==> completed!')\n",
    "\n",
    "    sorted_df_time2[COUNT] = e2\n",
    "    \n",
    "    e1 = time.time() - s1\n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sentiment\", IntegerType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"query\", StringType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"row text\", StringType())\n",
    "])\n",
    "\n",
    "df_main = spark.read.csv('../workTable/dataset/sentiment140-raw.csv', header = False, schema=schema)\n",
    "print(df_main.count())\n",
    "df_main = df_main.withColumnRenamed('row text', 'text')\n",
    "df_main = df_main.withColumnRenamed('sentiment', 'label')\n",
    "del_columns = ['date', 'query', 'user']\n",
    "df_main = df_main.drop(*del_columns)\n",
    "df_main = df_main.na.drop()\n",
    "df_main = df_main.na.drop(subset=[\"id\"])\n",
    "print(df_main.count())\n",
    "df_main.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_cassandra(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "\n",
    "def write_to_cassandra(df_sample, tabel, keyspace):\n",
    "    df_sample.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=tabel, keyspace=keyspace)\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cassandra_df_time = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    COUNT = df_sample.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    df_sample.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('overwrite')\\\n",
    "        .option(\"confirm.truncate\",\"true\")\\\n",
    "        .options(table='raw_temp', keyspace='sentiment_140')\\\n",
    "        .save()\n",
    "    \n",
    "    s1 = time.time()\n",
    "    df_sample = load_from_cassandra('sentiment_140' , 'raw_temp')\n",
    "    COUNT = df_sample.count()\n",
    "    print('   Cassandra table count : ' + str(COUNT))\n",
    "    tc = TwittCleaner(inputCol='text', outputCol='cleaned_text')\n",
    "    pipeline = Pipeline(stages=[tc])\n",
    "    pipiline_model = pipeline.fit(df_sample)\n",
    "    df_sample = pipiline_model.transform(df_sample)\n",
    "    df_sample.show(5)\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    cassandra_df_time[COUNT] = e1\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.drop('text')\n",
    "write_to_cassandra(df_sample, 'cleaned', 'sentiment_140')\n",
    "df_main = load_from_cassandra('sentiment_140', 'cleaned')\n",
    "df_count = df_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(review_text):\n",
    "    words = review_text.lower().split()\n",
    "    return words\n",
    "\n",
    "def analyze_texts(data_rdd):\n",
    "    def index(w_c_i):\n",
    "        ((w, c), i) = w_c_i\n",
    "        return (w, (i + 1, c))\n",
    "    return data_rdd.flatMap(lambda text_label: text_to_words(text_label[1])) \\\n",
    "        .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda w_c: - w_c[1]).zipWithIndex() \\\n",
    "        .map(lambda w_c_i: index(w_c_i)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cassandra_df_time2 = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    \n",
    "    COUNT = df_sample.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    df_sample.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('overwrite')\\\n",
    "        .option(\"confirm.truncate\",\"true\")\\\n",
    "        .options(table='cleaned_temp', keyspace='sentiment_140')\\\n",
    "        .save()\n",
    "    \n",
    "    s1 = time.time()\n",
    "    df_sample = load_from_cassandra('sentiment_140' , 'cleaned_temp')\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    df_rdd = df_sample.collect()\n",
    "    COUNT = df_sample.count()\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    print('   Cassandra table count : ' + str(COUNT))\n",
    "    \n",
    "    \n",
    "    print('   RDD is created ...')\n",
    "    \n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "\n",
    "    for row in df_rdd:    \n",
    "        if row[1] is not None:    \n",
    "            word_list = row[1].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(data_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1)+'-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1)+'-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "\n",
    "    \n",
    "    s2 = time.time()\n",
    "    w = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "    p = RPadd('words', 'padded', sequence_len, '##')\n",
    "    v = Vectorized('padded', 'vectorized', bfiltered_w2v.value)\n",
    "    pipeline = Pipeline(stages=[w, p, v])\n",
    "    pipiline_model = pipeline.fit(df_sample)\n",
    "    df_sample = pipiline_model.transform(df_sample)\n",
    "    df_sample.show(5)\n",
    "    e2 = (time.time() - s2) + e1\n",
    "    \n",
    "    cassandra_df_time2[COUNT] = e2\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e2) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sentiment\", IntegerType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"query\", StringType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"row text\", StringType())\n",
    "])\n",
    "\n",
    "df_main = spark.read.csv('../workTable/dataset/sentiment140-raw.csv', header = False, schema=schema)\n",
    "\n",
    "print(df_main.count())\n",
    "\n",
    "df_main = df_main.withColumnRenamed('row text', 'text')\n",
    "df_main = df_main.withColumnRenamed('sentiment', 'label')\n",
    "\n",
    "del_columns = ['date', 'query', 'user']\n",
    "df_main = df_main.drop(*del_columns)\n",
    "\n",
    "df_main = df_main.na.drop()\n",
    "df_main = df_main.na.drop(subset=[\"id\"])\n",
    "\n",
    "print(df_main.count())\n",
    "\n",
    "df_main.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mongo_df_time = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(0, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    COUNT = df_sample.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    df_sample.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "    \n",
    "    s1 = time.time()\n",
    "    sample = spark.read.format(\"mongo\").load()\n",
    "    COUNT = sample.count()\n",
    "    print('   Mongo table count : ' + str(COUNT))\n",
    "    tc = TwittCleaner(inputCol='text', outputCol='cleaned_text')\n",
    "    pipeline = Pipeline(stages=[tc])\n",
    "    pipiline_model = pipeline.fit(sample)\n",
    "    sample = pipiline_model.transform(sample)\n",
    "    sample.show(5)\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    mongo_df_time[COUNT] = e1\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e1) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(review_text):\n",
    "    words = review_text.lower().split()\n",
    "    return words\n",
    "\n",
    "def analyze_texts(data_rdd):\n",
    "    def index(w_c_i):\n",
    "        ((w, c), i) = w_c_i\n",
    "        return (w, (i + 1, c))\n",
    "    return data_rdd.flatMap(lambda text_label: text_to_words(text_label[3])) \\\n",
    "        .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda w_c: - w_c[1]).zipWithIndex() \\\n",
    "        .map(lambda w_c_i: index(w_c_i)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mongo_df_time2 = {}\n",
    "embedding_dim = 300\n",
    "\n",
    "for i in range(51, len(split_point)):\n",
    "    \n",
    "    print(str(i+1) + '.Example ' + str(i+1) + ' is started ...')\n",
    "    \n",
    "    df_sample = df_main.orderBy(rand()).sample(False, split_point[i], 10)\n",
    "    \n",
    "    COUNT = df_sample.count()\n",
    "    print('   DF count: ' + str(COUNT))\n",
    "    \n",
    "    df_sample.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "    \n",
    "    s1 = time.time()\n",
    "    sample = spark.read.format(\"mongo\").load()\n",
    "    e1 = time.time() - s1\n",
    "    \n",
    "    df_rdd = sample.collect()\n",
    "    COUNT = sample.count()\n",
    "    data_rdd = sc.parallelize(df_rdd, 200)\n",
    "    print('   Cassandra table count : ' + str(COUNT))\n",
    "    \n",
    "    \n",
    "    print('   RDD is created ...')\n",
    "    \n",
    "    seq_dict = {}\n",
    "    max_seq = 0\n",
    "\n",
    "    for row in df_rdd:    \n",
    "        if row[3] is not None:    \n",
    "            word_list = row[3].split(' ')       \n",
    "            seq_num = len(word_list)     \n",
    "            if seq_num in seq_dict.keys():\n",
    "                seq_dict[seq_num] += 1\n",
    "            else:\n",
    "                seq_dict[seq_num] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k,v in seq_dict.items():\n",
    "        if k >= max_seq:\n",
    "            max_seq = k\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    sequence_len = max_seq\n",
    "\n",
    "    print('   Max_seq : ' + str(max_seq))\n",
    "    \n",
    "    word_to_ic = analyze_texts(data_rdd)\n",
    "    word_to_ic = dict(word_to_ic)\n",
    "    print('   ' + str(i+1)+'-1.word_to_ic : ' + str(len(word_to_ic)))\n",
    "\n",
    "    filtered_w2v = {}\n",
    "\n",
    "    for w in list(word_to_ic):\n",
    "        if w in ft_model:\n",
    "            filtered_w2v[w] = ft_model.get_vector(w)\n",
    "        else:\n",
    "            word_to_ic.pop(w)\n",
    "        \n",
    "\n",
    "    print('   ' + str(i+1)+'-2.word_to_ic : ' + str(len(word_to_ic)))\n",
    "    print('   ' + str(i+1)+'-3.filtered_w2v : ' + str(len(filtered_w2v)))\n",
    "\n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "    \n",
    "    print('   Vectors extracted ...')\n",
    "\n",
    "    \n",
    "    s2 = time.time()\n",
    "    w = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    p = RPadd('words', 'padded', sequence_len, '##')\n",
    "    v = Vectorized('padded', 'vectorized', bfiltered_w2v.value)\n",
    "    pipeline = Pipeline(stages=[w, p, v])\n",
    "    pipiline_model = pipeline.fit(sample)\n",
    "    sample = pipiline_model.transform(sample)\n",
    "    sample.show(5)\n",
    "    e2 = (time.time() - s2) + e1\n",
    "    \n",
    "    mongo_df_time2[COUNT] = e2\n",
    "    \n",
    "    print('   Total Executaion Time: ' + str(e2) + ' s ==> completed!')\n",
    "    \n",
    "    print('*** *** *** ***')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gru'\n",
    "p = 0.2\n",
    "\n",
    "\n",
    "def build_model(class_num):\n",
    "    model = Sequential()\n",
    "    if model_type.lower() == \"cnn\":\n",
    "        model.add(Reshape([embedding_dim, 1, sequence_len]))\n",
    "        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n",
    "        model.add(ReLU())\n",
    "        model.add(SpatialMaxPooling(5, 1, 5, 1))\n",
    "        model.add(SpatialConvolution(128, 128, 5, 1))\n",
    "        model.add(ReLU())\n",
    "        model.add(SpatialMaxPooling(5, 1, 5, 1))\n",
    "        model.add(Reshape([128]))\n",
    "    \n",
    "    \n",
    "    elif model_type.lower() == \"lstm\":\n",
    "        model.add(Recurrent()\n",
    "                  .add(LSTM(embedding_dim, 128, p)))\n",
    "        model.add(Select(2, -1))\n",
    "    \n",
    "    \n",
    "    elif model_type.lower() == \"gru\":\n",
    "        model.add(Recurrent()\n",
    "                  .add(GRU(embedding_dim, 100, p)))\n",
    "        model.add(Select(2, -1))\n",
    "    \n",
    "    \n",
    "    elif model_type.lower() == \"bi\":\n",
    "        model.add(BiRecurrent(CAddTable()).add(RnnCell(300, 128, Tanh())))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Select(2, -1))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError('model can only be cnn, lstm, or gru')\n",
    "    \n",
    "    \n",
    "#     reshape_size = max_seq * 128\n",
    "#     print(reshape_size)\n",
    "    \n",
    "#     model.add(Reshape([reshape_size]))\n",
    "    model.add(Linear(100, 1))\n",
    "    model.add(LeakyReLU(negval=0.3,inplace=False,bigdl_type=\"float\"))\n",
    "    model.add(Dropout(0.3))\n",
    "#     model.add(Linear(100, 1))\n",
    "    model.add(Sigmoid()) \n",
    "    \n",
    "    \n",
    "#     model.add(Linear(100, class_num))\n",
    "#     model.add(LeakyReLU(negval=0.3,inplace=False,bigdl_type=\"float\"))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(SoftMax())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model,\n",
    "                     app_name,\n",
    "                     logdir='/tmp/bigdl_summaries',\n",
    "                     batch_size=batch_size,\n",
    "                     lr=0.01,\n",
    "                     lrd=0.0002,\n",
    "                     optim=\"Adam\",\n",
    "                     val=[\"Top1Accuracy\",\"Loss\"],\n",
    "                     max_epoch=30):\n",
    "    print(\"optimize summary will be write to :\",logdir+'/'+app_name)\n",
    "        \n",
    "    \n",
    "    #configure optimizer\n",
    "    optimizer = Optimizer(\n",
    "        model=model,\n",
    "        training_rdd=train_rdd,\n",
    "        criterion=BCECriterion(),\n",
    "        end_trigger=MaxEpoch(max_epoch),\n",
    "        batch_size=batch_size,\n",
    "        optim_method= Adam(learningrate=lr))\n",
    "\n",
    "#     optimizer.set_validation(\n",
    "#         batch_size=batch_size,\n",
    "#         val_rdd=val_rdd,\n",
    "#         trigger=EveryEpoch(),\n",
    "#         val_method=[Top1Accuracy()]\n",
    "#     )\n",
    "    \n",
    "#     train_summary = TrainSummary(log_dir=logdir, app_name=app_name)\n",
    "#     train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "#     val_summary = ValidationSummary(log_dir=logdir, app_name=app_name)\n",
    "#     optimizer.set_train_summary(train_summary)\n",
    "#     optimizer.set_val_summary(val_summary)\n",
    "    \n",
    "    train_summary = ''\n",
    "    val_summary = ''\n",
    "\n",
    "    return optimizer,train_summary,val_summary\n",
    "\n",
    "print('Start to train the model')\n",
    "#configure optimizer\n",
    "optimizer,train_summary,val_summary = create_optimizer(model,'adagrad-'+ dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"Optimization in progress ...\")\n",
    "trained_model = optimizer.optimize()\n",
    "print (\"Optimization Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluateResult = trained_model.evaluate(val_rdd, batch_size, [Top1Accuracy()])\n",
    "print(evaluateResult[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = trained_model.predict(val_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_predict_label(l):\n",
    "    if l >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0;\n",
    "def map_groundtruth_label(l):\n",
    "    return l[0] - 1\n",
    "\n",
    "y_pred = np.array([ map_predict_label(s) for s in predictions])\n",
    "\n",
    "y_true = np.array([s.labels[0].storage[0] for s in val_rdd.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"The prediction accuracy is %.2f%%\"%(acc*100))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(df_cm, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_model.saveModel(\"/home/mahdi/workTable/BigDL/model/model.bigdl\", \"/home/mahdi/workTable/BigDL/model/model.bin\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Loading model ...\")\n",
    "trained_model = Model.loadModel(\"/home/mahdi/workTable/BigDL/model/model.bigdl\", \"/home/mahdi/workTable/BigDL/model/model.bin\")\n",
    "print('Done!')"
   ]
  }
